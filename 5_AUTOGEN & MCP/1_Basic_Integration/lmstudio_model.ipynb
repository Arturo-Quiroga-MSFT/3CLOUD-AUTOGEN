{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80176a04",
   "metadata": {},
   "source": [
    "## interact with a locally hosted SLM (QWEN) to summarize web articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97be9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_core.models import ModelInfo , UserMessage \n",
    "from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2aa2091",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_model = OpenAIChatCompletionClient(\n",
    "        model=\"Qwen2.5-7B-Instruct-1M-iq3_xxs.gguf\",\n",
    "        base_url=\"http://localhost:8080/v1\",\n",
    "        model_info=ModelInfo(vision=False, function_calling=True, json_output=False, family=\"unknown\"),\n",
    "        api_key=\"lm-studio\",\n",
    "    ) \n",
    "# Setup server params for local filesystem access\n",
    "fetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n",
    "tools = await mcp_server_tools(fetch_mcp_server)\n",
    "\n",
    "result = await qwen_model.create(messages=[UserMessage(source=\"user\", content=\"Summarize the content of https://newsletter.victordibia.com/p/you-have-ai-fatigue-thats-why-you\")], tools=tools)\n",
    "\n",
    "print()\n",
    "print(result.content)\n",
    "print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8434659",
   "metadata": {},
   "source": [
    "## Additional interactions with a local SLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fce256f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"Mungert/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Introduce yourself.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=\"http://localhost:8080/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, introduce yourself to someone opening this program for the first time. Be concise.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"Mungert/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    import json\n",
    "    yellow_color = \"\\033[33m\"  # Yellow ANSI code\n",
    "    reset_color = \"\\033[0m\"\n",
    "    print(f\"{yellow_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    print(json.dumps(history, indent=2))\n",
    "    print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
