{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a40f97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#curl -fsSL https://ollama.com/install.sh | sh\n",
    "#%pip install 'litellm[proxy]'\n",
    "#!litellm --model ollama/llama3.2:1b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6982d000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "from autogen_core import (\n",
    "    AgentId,\n",
    "    DefaultTopicId,\n",
    "    MessageContext,\n",
    "    RoutedAgent,\n",
    "    SingleThreadedAgentRuntime,\n",
    "    default_subscription,\n",
    "    message_handler,\n",
    ")\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "from autogen_core.models import (\n",
    "    AssistantMessage,\n",
    "    ChatCompletionClient,\n",
    "    SystemMessage,\n",
    "    UserMessage,\n",
    ")\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f39b509e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_client() -> OpenAIChatCompletionClient:  # type: ignore\n",
    "    \"Mimic OpenAI API using Local LLM Server.\"\n",
    "    return OpenAIChatCompletionClient(\n",
    "        model=\"llama3.2:1b\",\n",
    "        api_key=\"NotRequiredSinceWeAreLocal\",\n",
    "        base_url=\"http://0.0.0.0:4000\",\n",
    "        model_capabilities={\n",
    "            \"json_output\": False,\n",
    "            \"vision\": False,\n",
    "            \"function_calling\": True,\n",
    "        },\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c15441",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e7fd803",
   "metadata": {},
   "outputs": [],
   "source": [
    "@default_subscription\n",
    "class Assistant(RoutedAgent):\n",
    "    def __init__(self, name: str, model_client: ChatCompletionClient) -> None:\n",
    "        super().__init__(\"An assistant agent.\")\n",
    "        self._model_client = model_client\n",
    "        self.name = name\n",
    "        self.count = 0\n",
    "        self._system_messages = [\n",
    "            SystemMessage(\n",
    "                content=f\"Your name is {name} and you are a part of a duo of comedians.\"\n",
    "                \"You laugh when you find the joke funny, else reply 'I need to go now'.\",\n",
    "            )\n",
    "        ]\n",
    "        self._model_context = BufferedChatCompletionContext(buffer_size=5)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_message(self, message: Message, ctx: MessageContext) -> None:\n",
    "        self.count += 1\n",
    "        await self._model_context.add_message(UserMessage(content=message.content, source=\"user\"))\n",
    "        result = await self._model_client.create(self._system_messages + await self._model_context.get_messages())\n",
    "\n",
    "        print(f\"\\n{self.name}: {message.content}\")\n",
    "\n",
    "        if \"I need to go\".lower() in message.content.lower() or self.count > 2:\n",
    "            return\n",
    "\n",
    "        await self._model_context.add_message(AssistantMessage(content=result.content, source=\"assistant\"))  # type: ignore\n",
    "        await self.publish_message(Message(content=result.content), DefaultTopicId())  # type: ignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8d1a156",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arturoquiroga/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py:413: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "runtime = SingleThreadedAgentRuntime()\n",
    "\n",
    "model_client = get_model_client()\n",
    "\n",
    "cathy = await Assistant.register(\n",
    "    runtime,\n",
    "    \"cathy\",\n",
    "    lambda: Assistant(name=\"Cathy\", model_client=model_client),\n",
    ")\n",
    "\n",
    "joe = await Assistant.register(\n",
    "    runtime,\n",
    "    \"joe\",\n",
    "    lambda: Assistant(name=\"Joe\", model_client=model_client),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f358c46",
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalServerError",
     "evalue": "Error code: 500 - {'error': {'message': 'litellm.APIConnectionError: OllamaException - {\"error\":\"model \\'llama3.2:1b\\' not found\"}', 'type': None, 'param': None, 'code': '500'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInternalServerError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m runtime.start()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runtime.send_message(\n\u001b[32m      3\u001b[39m     Message(\u001b[33m\"\u001b[39m\u001b[33mJoe, tell me a joke.\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      4\u001b[39m     recipient=AgentId(joe, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      5\u001b[39m     sender=AgentId(cathy, \u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m      6\u001b[39m )\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runtime.stop_when_idle()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Close the connections to the model clients.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py:332\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime.send_message\u001b[39m\u001b[34m(self, message, recipient, sender, cancellation_token, message_id)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._message_queue.put(\n\u001b[32m    319\u001b[39m     SendMessageEnvelope(\n\u001b[32m    320\u001b[39m         message=message,\n\u001b[32m   (...)\u001b[39m\u001b[32m    327\u001b[39m     )\n\u001b[32m    328\u001b[39m )\n\u001b[32m    330\u001b[39m cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m future\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_core/_single_threaded_agent_runtime.py:445\u001b[39m, in \u001b[36mSingleThreadedAgentRuntime._process_send\u001b[39m\u001b[34m(self, message_envelope)\u001b[39m\n\u001b[32m    443\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tracer_helper.trace_block(\u001b[33m\"\u001b[39m\u001b[33mprocess\u001b[39m\u001b[33m\"\u001b[39m, recipient_agent.id, parent=message_envelope.metadata):\n\u001b[32m    444\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m MessageHandlerContext.populate_context(recipient_agent.id):\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m             response = \u001b[38;5;28;01mawait\u001b[39;00m recipient_agent.on_message(\n\u001b[32m    446\u001b[39m                 message_envelope.message,\n\u001b[32m    447\u001b[39m                 ctx=message_context,\n\u001b[32m    448\u001b[39m             )\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CancelledError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    450\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message_envelope.future.cancelled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_core/_base_agent.py:113\u001b[39m, in \u001b[36mBaseAgent.on_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, message: Any, ctx: MessageContext) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_message_impl(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py:485\u001b[39m, in \u001b[36mRoutedAgent.on_message_impl\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[32m    484\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m h.router(message, ctx):\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m h(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.on_unhandled_message(message, ctx)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_core/_routed_agent.py:149\u001b[39m, in \u001b[36mmessage_handler.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    147\u001b[39m         logger.warning(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMessage type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in target types \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_types\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m return_value = \u001b[38;5;28;01mawait\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, message, ctx)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m AnyType \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(return_value) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m return_types:\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mAssistant.handle_message\u001b[39m\u001b[34m(self, message, ctx)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.count += \u001b[32m1\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_context.add_message(UserMessage(content=message.content, source=\u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_client.create(\u001b[38;5;28mself\u001b[39m._system_messages + \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._model_context.get_messages())\n\u001b[32m     22\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage.content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mI need to go\u001b[39m\u001b[33m\"\u001b[39m.lower() \u001b[38;5;129;01min\u001b[39;00m message.content.lower() \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.count > \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/autogen_ext/models/openai/_openai_client.py:624\u001b[39m, in \u001b[36mBaseOpenAIChatCompletionClient.create\u001b[39m\u001b[34m(self, messages, tools, json_output, extra_create_args, cancellation_token)\u001b[39m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    623\u001b[39m     cancellation_token.link_future(future)\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m result: Union[ParsedChatCompletion[BaseModel], ChatCompletion] = \u001b[38;5;28;01mawait\u001b[39;00m future\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m create_params.response_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    626\u001b[39m     result = cast(ParsedChatCompletion[Any], result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py:2028\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1985\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1986\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1987\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2025\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   2026\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2027\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2028\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2029\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2030\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2031\u001b[39m             {\n\u001b[32m   2032\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2033\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2034\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2035\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2036\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2037\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2038\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2039\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2040\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2041\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2042\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2043\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2044\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2045\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2046\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2047\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2048\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2049\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2050\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2051\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2052\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2053\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2054\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2055\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2056\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2057\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2058\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2059\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2060\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2061\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2062\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2063\u001b[39m             },\n\u001b[32m   2064\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2065\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2066\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2067\u001b[39m         ),\n\u001b[32m   2068\u001b[39m         options=make_request_options(\n\u001b[32m   2069\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2070\u001b[39m         ),\n\u001b[32m   2071\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2072\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2073\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2074\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/openai/_base_client.py:1742\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1728\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1729\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1730\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1737\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1738\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1739\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1740\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1741\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/3CLOUD-AUTOGEN/.venv/lib/python3.13/site-packages/openai/_base_client.py:1549\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1546\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1548\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1549\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1553\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mInternalServerError\u001b[39m: Error code: 500 - {'error': {'message': 'litellm.APIConnectionError: OllamaException - {\"error\":\"model \\'llama3.2:1b\\' not found\"}', 'type': None, 'param': None, 'code': '500'}}"
     ]
    }
   ],
   "source": [
    "runtime.start()\n",
    "await runtime.send_message(\n",
    "    Message(\"Joe, tell me a joke.\"),\n",
    "    recipient=AgentId(joe, \"default\"),\n",
    "    sender=AgentId(cathy, \"default\"),\n",
    ")\n",
    "await runtime.stop_when_idle()\n",
    "\n",
    "# Close the connections to the model clients.\n",
    "await model_client.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
